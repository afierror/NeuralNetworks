{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AND_NN","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"S89DqgtGa6X_","colab_type":"code","colab":{}},"source":["#Red neuronal \n","#Compuerta lÃ³gica AND\n","#Atoany Fierro\n","\n","import numpy as np\n","\n","# X = (hours studying, hours sleeping), y = score on test\n","xAll = np.array(([0, 0], [0, 1], [3, 6], [5, 10]), dtype=float) # input data\n","y = np.array(([92], [86], [89]), dtype=float) # output\n","\n","# scale units\n","xAll = xAll/np.amax(xAll, axis=0) # scaling input data\n","y = y/100 # scaling output data (max test score is 100)\n","\n","# split data\n","X = np.split(xAll, [3])[0] # training data\n","xPredicted = np.split(xAll, [3])[1] # testing data\n","\n","class Neural_Network(object):\n","  def __init__(self):\n","  #parameters\n","    self.inputSize = 2\n","    self.outputSize = 1\n","    self.hiddenSize = 3\n","\n","  #weights\n","    self.W1 = np.random.randn(self.inputSize, self.hiddenSize) # (3x2) weight matrix from input to hidden layer\n","    self.W2 = np.random.randn(self.hiddenSize, self.outputSize) # (3x1) weight matrix from hidden to output layer\n","\n","  def forward(self, X):\n","    #forward propagation through our network\n","    self.z = np.dot(X, self.W1) # dot product of X (input) and first set of 3x2 weights\n","    self.z2 = self.sigmoid(self.z) # activation function\n","    self.z3 = np.dot(self.z2, self.W2) # dot product of hidden layer (z2) and second set of 3x1 weights\n","    o = self.sigmoid(self.z3) # final activation function\n","    return o\n","\n","  def sigmoid(self, s):\n","    # activation function\n","    return 1/(1+np.exp(-s))\n","\n","  def sigmoidPrime(self, s):\n","    #derivative of sigmoid\n","    return s * (1 - s)\n","\n","  def backward(self, X, y, o):\n","    # backward propagate through the network\n","    self.o_error = y - o # error in output\n","    self.o_delta = self.o_error*self.sigmoidPrime(o) # applying derivative of sigmoid to error\n","\n","    self.z2_error = self.o_delta.dot(self.W2.T) # z2 error: how much our hidden layer weights contributed to output error\n","    self.z2_delta = self.z2_error*self.sigmoidPrime(self.z2) # applying derivative of sigmoid to z2 error\n","\n","    self.W1 += X.T.dot(self.z2_delta) # adjusting first set (input --> hidden) weights\n","    self.W2 += self.z2.T.dot(self.o_delta) # adjusting second set (hidden --> output) weights\n","\n","  def train(self, X, y):\n","    o = self.forward(X)\n","    self.backward(X, y, o)\n","\n","  def saveWeights(self):\n","    np.savetxt(\"w1.txt\", self.W1, fmt=\"%s\")\n","    np.savetxt(\"w2.txt\", self.W2, fmt=\"%s\")\n","\n","  def predict(self):\n","    print (\"Predicted data based on trained weights: \");\n","    print (\"Input (scaled): \" + str(xPredicted));\n","    print (\"Output: \" + str(self.forward(xPredicted)));\n","\n","NN = Neural_Network()\n","for i in range(1000): # trains the NN 1,000 times\n","  print (\"# \" + str(i))\n","  print (\"Input (scaled): \" + str(X))\n","  print (\"Actual Output: \" + str(y))\n","  print (\"Predicted Output: \" + str(NN.forward(X)))\n","  print (\"Loss: \" + str(np.mean(np.square(y - NN.forward(X))))) # mean sum squared loss\n","  print ()\n","  NN.train(X, y)\n","\n","NN.saveWeights()\n","NN.predict()\n","\n","\n","\n"],"execution_count":0,"outputs":[]}]}